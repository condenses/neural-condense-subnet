from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
import torch
from typing import List
import random
from semantic_text_splitter import TextSplitter
from copy import deepcopy
from datasets import load_dataset

negative_dataset = load_dataset("Salesforce/wikitext", streaming=True, split="train")
negative_dataset = negative_dataset.shuffle()
negative_dataset = negative_dataset.filter(
    lambda x: len(x["text"]) > 100 and len(x["text"]) < 2048
)

get_negative_message = lambda: next(negative_dataset)["text"]

splitter = TextSplitter(256)


def check_text_exists(
    tokenizer: AutoTokenizer,
    model: AutoModelForCausalLM,
    kv_cache: DynamicCache,
    query_chunk: str,
) -> bool:
    _kv_cache = deepcopy(kv_cache)
    prompt = f"</s> [INST] Retrieve from the history of the conversation, check if the following sentence is present:\n```\n{query_chunk}\n```\n Only answer with 'yes' or 'no'. No required explanation. [/INST] "
    prompt_ids = tokenizer.encode(
        prompt,
        return_tensors="pt",
        add_special_tokens=False,
    )
    padded_prompt_ids = torch.cat(
        [
            torch.full((1, _kv_cache._seen_tokens), 0).to(dtype=torch.long),
            prompt_ids,
        ],
        dim=1,
    ).to(device=model.device, dtype=torch.long)
    print(f"Prompt: {prompt}")

    outputs = model.generate(
        input_ids=padded_prompt_ids,
        past_key_values=_kv_cache,
        max_new_tokens=16,
    )
    completion_text = tokenizer.decode(
        outputs[0][padded_prompt_ids.shape[1] :], skip_special_tokens=True
    )
    print(f"Filter Completion: {completion_text}")
    return "yes" in completion_text.lower()


def filter_existance(
    tokenizer: AutoTokenizer,
    model: AutoModelForCausalLM,
    kv_cache: DynamicCache,
    messages: List[dict],
    hidden_messages: List[dict],
) -> bool:
    # Test on positive case (text from conversation)
    contents = [msg["content"] for msg in messages + hidden_messages]
    random_message = random.choice(contents)
    chunks = splitter.chunks(random_message)
    query_chunk = random.choice(chunks)
    if not check_text_exists(tokenizer, model, kv_cache, query_chunk):
        return False

    # Test on negative case (text not from conversation)
    negative_chunk = random.choice(splitter.chunks(get_negative_message()))
    print(f"Negative chunk: {negative_chunk}")
    if check_text_exists(tokenizer, model, kv_cache, negative_chunk):
        return False

    return True
