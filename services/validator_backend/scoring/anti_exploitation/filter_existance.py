from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
import torch
from typing import List
import random
from semantic_text_splitter import TextSplitter
from copy import deepcopy
from datasets import load_dataset
from typing import Tuple


class FilterExistanceChecker:
    def __init__(self):
        self.splitter = TextSplitter(256)
        self.negative_dataset = self._load_negative_dataset()

    def _load_negative_dataset(self):
        negative_dataset = load_dataset(
            "TIGER-Lab/Fineweb-Instruct", streaming=True, split="train"
        )
        negative_dataset = negative_dataset.shuffle()
        negative_dataset = negative_dataset.filter(lambda x: len(x["response"]) > 100)
        negative_dataset = negative_dataset.map(lambda x: {"text": x["response"]})
        negative_dataset = iter(negative_dataset)
        return negative_dataset

    def _get_negative_message(self):
        try:
            return next(self.negative_dataset)["text"]
        except StopIteration:
            self.negative_dataset = self._load_negative_dataset()
            return self._get_negative_message()

    def _check_text_exists(
        self,
        tokenizer: AutoTokenizer,
        model: AutoModelForCausalLM,
        kv_cache: DynamicCache,
        query_chunk: str,
    ) -> bool:
        _kv_cache = deepcopy(kv_cache)
        prompt = f"</s> [INST] Retrieve from the history of the conversation, check if the following sentence is existed in the conversation. Only answer with 'yes' or 'no'. No required explanation.\n```\n{query_chunk}\n```\n [/INST]"
        prompt_ids = tokenizer.encode(
            prompt,
            return_tensors="pt",
            add_special_tokens=False,
        )
        padded_prompt_ids = torch.cat(
            [
                torch.full((1, _kv_cache._seen_tokens), 0).to(dtype=torch.long),
                prompt_ids,
            ],
            dim=1,
        ).to(device=model.device, dtype=torch.long)
        print(f"Prompt: {prompt}")

        outputs = model.generate(
            input_ids=padded_prompt_ids,
            past_key_values=_kv_cache,
            max_new_tokens=32,
        )
        completion_text = tokenizer.decode(
            outputs[0][padded_prompt_ids.shape[1] :], skip_special_tokens=True
        )
        print(f"Filter Completion: {completion_text}")
        return "yes" in completion_text.lower(), "no" in completion_text.lower()

    def get_messages_pair(
        self, messages: List[dict], hidden_messages: List[dict]
    ) -> Tuple[str, str]:
        # Test on positive case (text from conversation)
        contents = [msg["content"] for msg in messages + hidden_messages]
        random_message = random.choice(contents)
        chunks = self.splitter.chunks(random_message)
        positive_chunk = random.choice(chunks)
        # Test on negative case (text not from conversation)
        negative_chunk = random.choice(
            self.splitter.chunks(self._get_negative_message())
        )
        return positive_chunk, negative_chunk

    def filter_existance(
        self,
        tokenizer: AutoTokenizer,
        model: AutoModelForCausalLM,
        kv_cache: DynamicCache,
        positive_chunk: str,
        negative_chunk: str,
    ) -> bool:
        exist_yes, exist_no = self._check_text_exists(
            tokenizer, model, kv_cache, positive_chunk
        )
        if not exist_yes or (exist_yes and exist_no):
            return False

        # Test on negative case (text not from conversation)
        print(f"Negative chunk: {negative_chunk}")
        exist_yes, exists_no = self._check_text_exists(
            tokenizer, model, kv_cache, negative_chunk
        )
        if not exists_no or (exist_yes and exists_no):
            return False

        return True
