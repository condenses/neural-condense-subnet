from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
import torch
from typing import List
import random
from semantic_text_splitter import TextSplitter
from copy import deepcopy

splitter = TextSplitter(256)


def filter_existance(
    tokenizer: AutoTokenizer,
    model: AutoModelForCausalLM,
    kv_cache: DynamicCache,
    messages: List[str],
    hidden_messages: List[str],
) -> bool:
    _kv_cache = deepcopy(kv_cache)
    random_message = random.choice(messages)
    # Select a random sentence from the message
    chunks = splitter.split(random_message)
    query_chunk = random.choice(chunks)
    prompt = f"Retrieve from the history of the conversation, check if the following sentence is present:\n```\n{query_chunk}\n```\n Only answer with 'yes' or 'no'. No required explanation."
    prompt_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
    padded_prompt_ids = torch.cat(
        [
            torch.full((1, _kv_cache._seen_tokens), 0),
            prompt_ids,
        ]
    )

    outputs = model.generate(
        input_ids=padded_prompt_ids,
        past_key_values=_kv_cache,
        max_new_tokens=16,
    )
    completion_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return "yes" in completion_text.lower()
