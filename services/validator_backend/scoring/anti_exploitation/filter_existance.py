from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache
import torch
from typing import List
import random
from semantic_text_splitter import TextSplitter
from copy import deepcopy

splitter = TextSplitter(256)


def filter_existance(
    tokenizer: AutoTokenizer,
    model: AutoModelForCausalLM,
    kv_cache: DynamicCache,
    messages: List[dict],
    hidden_messages: List[dict],
) -> bool:
    _kv_cache = deepcopy(kv_cache)
    contents = [msg["content"] for msg in messages + hidden_messages]
    random_message = random.choice(contents)
    # Select a random sentence from the message
    chunks = splitter.chunks(random_message)
    query_chunk = random.choice(chunks)
    prompt = f"Retrieve from the history of the conversation, check if the following sentence is present:\n```\n{query_chunk}\n```\n Only answer with 'yes' or 'no'. No required explanation."
    prompt_ids = tokenizer.encode(
        prompt,
        return_tensors="pt",
        add_special_tokens=False,
    )
    padded_prompt_ids = torch.cat(
        [
            torch.full((1, _kv_cache._seen_tokens), 0).to(
                device=model.device, dtype=torch.long
            ),
            prompt_ids,
        ],
        dim=1,
    ).to(device=model.device, dtype=torch.long)
    print(f"Prompt: {prompt}")

    outputs = model.generate(
        input_ids=padded_prompt_ids,
        past_key_values=_kv_cache,
        max_new_tokens=16,
    )
    completion_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return "yes" in completion_text.lower()
